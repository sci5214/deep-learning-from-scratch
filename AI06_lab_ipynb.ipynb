{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "name": "AI06_lab.ipynb의 사본",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sci5214/deep-learning-from-scratch/blob/master/AI06_lab_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWn6DjEWPIf3",
        "colab_type": "text"
      },
      "source": [
        "# AI 06\n",
        "\n",
        "*6.5, 6.6, 6.7, 6.8 만 코드 타이핑이며 나머지 코드는 실행결과만 확인*\n",
        "\n",
        "> 학번: 2019192052\n",
        ">\n",
        "> 이름: 김민서\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6rQXwzZPIf3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgHlhJ_7PIf6",
        "colab_type": "text"
      },
      "source": [
        "### 매개변수 갱신\n",
        "___"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6DzugPoPIf7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 6.1 SGD\n",
        "\n",
        "class SGD:\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr\n",
        "        \n",
        "    def update(self, params, grads):\n",
        "        for key in params.keys():\n",
        "            params[key] -= self.lr * grads[key]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlBqQD6fPIf9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 6.2 Momentum\n",
        "\n",
        "class Momentum:\n",
        "    def __init__(self, lr=0.01, momentum=0.9):\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.v = None\n",
        "        \n",
        "    def update(self, param, grad):\n",
        "        if self.v is None:\n",
        "            self.v = {}\n",
        "            for key, val in params.items():\n",
        "                self.v[key] = np.zeros_like(val)\n",
        "\n",
        "        for key in params.keys():\n",
        "            self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]\n",
        "            params[key] += self.v[key]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGaG6dg7PIf_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 6.3 AdaGrad\n",
        "\n",
        "class AdaGrad:\n",
        "\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr\n",
        "        self.h = None\n",
        "        \n",
        "    def update(self, params, grads):\n",
        "        if self.h is None:\n",
        "            self.h = {}\n",
        "            for key, val in params.items():\n",
        "                self.h[key] = np.zeros_like(val)\n",
        "            \n",
        "        for key in params.keys():\n",
        "            self.h[key] += grads[key] * grads[key]\n",
        "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvBE-KC8PIgB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Nesterov\n",
        "\n",
        "class Nesterov:\n",
        "\n",
        "    def __init__(self, lr=0.01, momentum=0.9):\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.v = None\n",
        "        \n",
        "    def update(self, params, grads):\n",
        "        if self.v is None:\n",
        "            self.v = {}\n",
        "            for key, val in params.items():\n",
        "                self.v[key] = np.zeros_like(val)\n",
        "            \n",
        "        for key in params.keys():\n",
        "            self.v[key] *= self.momentum\n",
        "            self.v[key] -= self.lr * grads[key]\n",
        "            params[key] += self.momentum * self.momentum * self.v[key]\n",
        "            params[key] -= (1 + self.momentum) * self.lr * grads[key]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gH4rRga4PIgE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# RMSprop\n",
        "\n",
        "class RMSprop:\n",
        "\n",
        "    def __init__(self, lr=0.01, decay_rate = 0.99):\n",
        "        self.lr = lr\n",
        "        self.decay_rate = decay_rate\n",
        "        self.h = None\n",
        "        \n",
        "    def update(self, params, grads):\n",
        "        if self.h is None:\n",
        "            self.h = {}\n",
        "            for key, val in params.items():\n",
        "                self.h[key] = np.zeros_like(val)\n",
        "            \n",
        "        for key in params.keys():\n",
        "            self.h[key] *= self.decay_rate\n",
        "            self.h[key] += (1 - self.decay_rate) * grads[key] * grads[key]\n",
        "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqxUW6Z6PIgH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Adam\n",
        "\n",
        "class Adam:\n",
        "\n",
        "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
        "        self.lr = lr\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.iter = 0\n",
        "        self.m = None\n",
        "        self.v = None\n",
        "        \n",
        "    def update(self, params, grads):\n",
        "        if self.m is None:\n",
        "            self.m, self.v = {}, {}\n",
        "            for key, val in params.items():\n",
        "                self.m[key] = np.zeros_like(val)\n",
        "                self.v[key] = np.zeros_like(val)\n",
        "        \n",
        "        self.iter += 1\n",
        "        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)         \n",
        "        \n",
        "        for key in params.keys():\n",
        "            #self.m[key] = self.beta1*self.m[key] + (1-self.beta1)*grads[key]\n",
        "            #self.v[key] = self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)\n",
        "            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
        "            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n",
        "            \n",
        "            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)\n",
        "            \n",
        "            #unbias_m += (1 - self.beta1) * (grads[key] - self.m[key]) # correct bias\n",
        "            #unbisa_b += (1 - self.beta2) * (grads[key]*grads[key] - self.v[key]) # correct bias\n",
        "            #params[key] += self.lr * unbias_m / (np.sqrt(unbisa_b) + 1e-7)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DahRxGXMPIgK",
        "colab_type": "text"
      },
      "source": [
        "### 가중치의 초깃값\n",
        "___"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2lWbacuPIgK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def ReLU(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def softmax(x):\n",
        "    if x.ndim == 2:\n",
        "        x = x.T\n",
        "        x = x - np.max(x, axis=0)\n",
        "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "        return y.T \n",
        "\n",
        "    x = x - np.max(x)\n",
        "    return np.exp(x) / np.sum(np.exp(x))\n",
        "\n",
        "def cross_entropy_error(y, t):\n",
        "    if y.ndim == 1:\n",
        "        t = t.reshape(1, t.size)\n",
        "        y = y.reshape(1, y.size)\n",
        "        \n",
        "    if t.size == y.size:\n",
        "        t = t.argmax(axis=1)\n",
        "             \n",
        "    batch_size = y.shape[0]\n",
        "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "CXOcsiVfPIgM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 6.4\n",
        "\n",
        "x = input_data = np.random.randn(1000, 100)\n",
        "node_num = 100\n",
        "hidden_layer_size = 5\n",
        "activations = {}\n",
        "\n",
        "for i in range(hidden_layer_size):\n",
        "\n",
        "    if i != 0:\n",
        "        x = activations[i-1]\n",
        "\n",
        "    w = np.random.randn(node_num, node_num) * 1\n",
        "\n",
        "    a = np.dot(x, w)\n",
        "    \n",
        "    z = sigmoid(a)\n",
        "\n",
        "    activations[i] = z\n",
        "\n",
        "plt.figure(figsize=(12,3))\n",
        "for i, a in activations.items():\n",
        "    plt.subplot(1, len(activations), i+1)\n",
        "    plt.title(str(i+1) + \"-layer\")\n",
        "    if i != 0: plt.yticks([], [])\n",
        "    # plt.xlim(0.1, 1)\n",
        "    # plt.ylim(0, 7000)\n",
        "    plt.hist(a.flatten(), 30, range=(0,1))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0kjroPUPIgO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 6.5 (# 6.4에서 표준편차만 0.01로 변경)\n",
        "# 6.4\n",
        "\n",
        "x = input_data = np.random.randn(1000, 100)\n",
        "node_num = 100\n",
        "hidden_layer_size = 5\n",
        "activations = {}\n",
        "\n",
        "for i in range(hidden_layer_size):\n",
        "\n",
        "    if i != 0:\n",
        "        x = activations[i-1]\n",
        "\n",
        "    w = np.random.randn(node_num, node_num) * 0.01\n",
        "\n",
        "    a = np.dot(x, w)\n",
        "    \n",
        "    z = sigmoid(a)\n",
        "\n",
        "    activations[i] = z\n",
        "\n",
        "plt.figure(figsize=(12,3))\n",
        "for i, a in activations.items():\n",
        "    plt.subplot(1, len(activations), i+1)\n",
        "    plt.title(str(i+1) + \"-layer\")\n",
        "    if i != 0: plt.yticks([], [])\n",
        "    # plt.xlim(0.1, 1)\n",
        "    # plt.ylim(0, 7000)\n",
        "    plt.hist(a.flatten(), 30, range=(0,1))\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxBhfl2QPIgR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 6.6 (# 6.5에서 표준편차를 1/sqrt(node_num)으로 변경 )\n",
        "\n",
        "x = input_data = np.random.randn(1000, 100)\n",
        "node_num = 100\n",
        "hidden_layer_size = 5\n",
        "activations = {}\n",
        "\n",
        "for i in range(hidden_layer_size):\n",
        "\n",
        "    if i != 0:\n",
        "        x = activations[i-1]\n",
        "\n",
        "    w = np.random.randn(node_num, node_num) / sqrt(node_num)\n",
        "\n",
        "    a = np.dot(x, w)\n",
        "    \n",
        "    z = sigmoid(a)\n",
        "\n",
        "    activations[i] = z\n",
        "\n",
        "plt.figure(figsize=(12,3))\n",
        "for i, a in activations.items():\n",
        "    plt.subplot(1, len(activations), i+1)\n",
        "    plt.title(str(i+1) + \"-layer\")\n",
        "    if i != 0: plt.yticks([], [])\n",
        "    # plt.xlim(0.1, 1)\n",
        "    # plt.ylim(0, 7000)\n",
        "    plt.hist(a.flatten(), 30, range=(0,1))\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "SozoZANnPIgW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 6.7 (# 6.6에서 활성화 함수만 simoid에서 tanh로 변경)\n",
        "\n",
        "x = input_data = np.random.randn(1000, 100)\n",
        "node_num = 100\n",
        "hidden_layer_size = 5\n",
        "activations = {}\n",
        "\n",
        "for i in range(hidden_layer_size):\n",
        "\n",
        "    if i != 0:\n",
        "        x = activations[i-1]\n",
        "\n",
        "    w = np.random.randn(node_num, node_num) / sqrt(node_num)\n",
        "\n",
        "    a = np.dot(x, w)\n",
        "    \n",
        "    z = tanh(a)\n",
        "\n",
        "    activations[i] = z\n",
        "\n",
        "plt.figure(figsize=(12,3))\n",
        "for i, a in activations.items():\n",
        "    plt.subplot(1, len(activations), i+1)\n",
        "    plt.title(str(i+1) + \"-layer\")\n",
        "    if i != 0: plt.yticks([], [])\n",
        "    # plt.xlim(0.1, 1)\n",
        "    # plt.ylim(0, 7000)\n",
        "    plt.hist(a.flatten(), 30, range=(0,1))\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKdgW_wjPIgY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 6.8 He 초깃값 (# 6.7에서 표준편차만 sqrt(2/node_num)으로 변경)\n",
        "\n",
        "x = input_data = np.random.randn(1000, 100)\n",
        "node_num = 100\n",
        "hidden_layer_size = 5\n",
        "activations = {}\n",
        "\n",
        "for i in range(hidden_layer_size):\n",
        "\n",
        "    if i != 0:\n",
        "        x = activations[i-1]\n",
        "\n",
        "    w = np.random.randn(node_num, node_num) * sqrt(2/node_num)\n",
        "\n",
        "    a = np.dot(x, w)\n",
        "    \n",
        "    z = tanh(a)\n",
        "\n",
        "    activations[i] = z\n",
        "\n",
        "plt.figure(figsize=(12,3))\n",
        "for i, a in activations.items():\n",
        "    plt.subplot(1, len(activations), i+1)\n",
        "    plt.title(str(i+1) + \"-layer\")\n",
        "    if i != 0: plt.yticks([], [])\n",
        "    # plt.xlim(0.1, 1)\n",
        "    # plt.ylim(0, 7000)\n",
        "    plt.hist(a.flatten(), 30, range=(0,1))\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9V_BM-sPIga",
        "colab_type": "text"
      },
      "source": [
        "### 배치 정규화\n",
        "___"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yg7zeUCYPIgb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 6.8 MNIST Dataset\n",
        "\n",
        "from sklearn.datasets import fetch_openml\n",
        "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
        "\n",
        "X = X.astype(np.float) / 255.\n",
        "y = y.astype(np.uint8)\n",
        "\n",
        "x_train = X[:60000]\n",
        "x_test = X[60000:]\n",
        "t_train = y[:60000]\n",
        "t_test = y[60000:]\n",
        "\n",
        "print(x_train.shape)\n",
        "print(t_train.shape)\n",
        "print(x_test.shape)\n",
        "print(t_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaWDwdP0PIgd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 레이어 클래스\n",
        "\n",
        "class Sigmoid:\n",
        "    def __init__(self):\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = sigmoid(x)\n",
        "        self.out = out\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout * (1.0 - self.out) * self.out\n",
        "\n",
        "        return dx\n",
        "    \n",
        "    \n",
        "class Relu:\n",
        "    def __init__(self):\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.mask = (x <= 0)\n",
        "        out = x.copy()\n",
        "        out[self.mask] = 0\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout[self.mask] = 0\n",
        "        dx = dout\n",
        "\n",
        "        return dx\n",
        "    \n",
        "\n",
        "class Affine:\n",
        "    def __init__(self, W, b):\n",
        "        self.W =W\n",
        "        self.b = b\n",
        "        \n",
        "        self.x = None\n",
        "        self.original_x_shape = None\n",
        "\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        self.original_x_shape = x.shape\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        self.x = x\n",
        "\n",
        "        out = np.dot(self.x, self.W) + self.b\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = np.dot(dout, self.W.T)\n",
        "        self.dW = np.dot(self.x.T, dout)\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "        \n",
        "        dx = dx.reshape(*self.original_x_shape)\n",
        "        return dx    \n",
        "    \n",
        "\n",
        "class BatchNormalization:\n",
        "\n",
        "    def __init__(self, gamma, beta, momentum=0.9, running_mean=None, running_var=None):\n",
        "        self.gamma = gamma\n",
        "        self.beta = beta\n",
        "        self.momentum = momentum\n",
        "        self.input_shape = None \n",
        "\n",
        "        self.running_mean = running_mean\n",
        "        self.running_var = running_var  \n",
        "        \n",
        "        self.batch_size = None\n",
        "        self.xc = None\n",
        "        self.std = None\n",
        "        self.dgamma = None\n",
        "        self.dbeta = None\n",
        "\n",
        "    def forward(self, x, train_flg=True):\n",
        "        self.input_shape = x.shape\n",
        "        if x.ndim != 2:\n",
        "            N, C, H, W = x.shape\n",
        "            x = x.reshape(N, -1)\n",
        "\n",
        "        out = self.__forward(x, train_flg)\n",
        "        \n",
        "        return out.reshape(*self.input_shape)\n",
        "            \n",
        "    def __forward(self, x, train_flg):\n",
        "        if self.running_mean is None:\n",
        "            N, D = x.shape\n",
        "            self.running_mean = np.zeros(D)\n",
        "            self.running_var = np.zeros(D)\n",
        "                        \n",
        "        if train_flg:\n",
        "            mu = x.mean(axis=0)\n",
        "            xc = x - mu\n",
        "            var = np.mean(xc**2, axis=0)\n",
        "            std = np.sqrt(var + 10e-7)\n",
        "            xn = xc / std\n",
        "            \n",
        "            self.batch_size = x.shape[0]\n",
        "            self.xc = xc\n",
        "            self.xn = xn\n",
        "            self.std = std\n",
        "            self.running_mean = self.momentum * self.running_mean + (1-self.momentum) * mu\n",
        "            self.running_var = self.momentum * self.running_var + (1-self.momentum) * var            \n",
        "        else:\n",
        "            xc = x - self.running_mean\n",
        "            xn = xc / ((np.sqrt(self.running_var + 10e-7)))\n",
        "            \n",
        "        out = self.gamma * xn + self.beta \n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        if dout.ndim != 2:\n",
        "            N, C, H, W = dout.shape\n",
        "            dout = dout.reshape(N, -1)\n",
        "\n",
        "        dx = self.__backward(dout)\n",
        "\n",
        "        dx = dx.reshape(*self.input_shape)\n",
        "        return dx\n",
        "\n",
        "    def __backward(self, dout):\n",
        "        dbeta = dout.sum(axis=0)\n",
        "        dgamma = np.sum(self.xn * dout, axis=0)\n",
        "        dxn = self.gamma * dout\n",
        "        dxc = dxn / self.std\n",
        "        dstd = -np.sum((dxn * self.xc) / (self.std * self.std), axis=0)\n",
        "        dvar = 0.5 * dstd / self.std\n",
        "        dxc += (2.0 / self.batch_size) * self.xc * dvar\n",
        "        dmu = np.sum(dxc, axis=0)\n",
        "        dx = dxc - dmu / self.batch_size\n",
        "        \n",
        "        self.dgamma = dgamma\n",
        "        self.dbeta = dbeta\n",
        "        \n",
        "        return dx\n",
        "    \n",
        "\n",
        "class SoftmaxWithLoss:\n",
        "    def __init__(self):\n",
        "        self.loss = None\n",
        "        self.y = None\n",
        "        self.t = None\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        self.t = t\n",
        "        self.y = softmax(x)\n",
        "        self.loss = cross_entropy_error(self.y, self.t)\n",
        "        \n",
        "        return self.loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        batch_size = self.t.shape[0]\n",
        "        if self.t.size == self.y.size:\n",
        "            dx = (self.y - self.t) / batch_size\n",
        "        else:\n",
        "            dx = self.y.copy()\n",
        "            dx[np.arange(batch_size), self.t] -= 1\n",
        "            dx = dx / batch_size\n",
        "        \n",
        "        return dx\n",
        "    \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEtjgGiePIge",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MultiLayerNet\n",
        "\n",
        "from collections import OrderedDict\n",
        "\n",
        "class MultiLayerNet:\n",
        "    \"\"\"\n",
        "    Weight Decay, Dropout,Batch Normalization\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_size : 입력 노드 갯수\n",
        "    hidden_size_list : 은닉층 노드 갯수 리스트 （e.g. [100, 100, 100]）\n",
        "    output_size : 출력 노드 갯수\n",
        "    activation : 'relu' or 'sigmoid'\n",
        "    weight_init_std : 초깃값 표준편차 （e.g. 0.01）\n",
        "        'relu' 또는 'he'를 지정할 경우 he 초깃값 설정\n",
        "        'sigmoid' 또는 'xavier'를 지정할 경우 xavier 초깃값 설정\n",
        "    weight_decay_lambda : Weight Decay（L2 norm）\n",
        "    use_dropout: Dropout 사용 여부\n",
        "    dropout_ratio : Dropout 비율\n",
        "    use_batchNorm: Batch Normalization 사용 여부\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size, hidden_size_list, output_size,\n",
        "                 activation='relu', weight_init_std='relu', weight_decay_lambda=0, \n",
        "                 use_dropout = False, dropout_ratio = 0.5, use_batchnorm=False):\n",
        "        \n",
        "        # 파라미터 저장\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.hidden_size_list = hidden_size_list\n",
        "        self.hidden_layer_num = len(hidden_size_list)\n",
        "        self.use_dropout = use_dropout\n",
        "        self.weight_decay_lambda = weight_decay_lambda\n",
        "        self.use_batchnorm = use_batchnorm\n",
        "\n",
        "        # 가중치 초기화        \n",
        "        self.params = {}\n",
        "        all_size_list = [self.input_size] + self.hidden_size_list + [self.output_size]\n",
        "        for idx in range(1, len(all_size_list)):\n",
        "            scale = weight_init_std\n",
        "            # 만약 weight_init_std가 relu나 he일 경우\n",
        "            if str(weight_init_std).lower() in ('relu', 'he'):\n",
        "                # He 초깃값 적용\n",
        "                scale = np.sqrt(2.0 / all_size_list[idx - 1])\n",
        "            # 만약 weight_init_std가 sigmoid나 zavier의 경우\n",
        "            elif str(weight_init_std).lower() in ('sigmoid', 'xavier'):\n",
        "                # Xavier 초깃값 적용\n",
        "                scale = np.sqrt(1.0 / all_size_list[idx - 1])\n",
        "            # 가중치 초기화\n",
        "            self.params['W' + str(idx)] = scale * np.random.randn(all_size_list[idx-1], all_size_list[idx])\n",
        "            self.params['b' + str(idx)] = np.zeros(all_size_list[idx])\n",
        "\n",
        "        # layers 초기화\n",
        "        activation_layer = {'sigmoid': Sigmoid, 'relu': Relu}\n",
        "        self.layers = OrderedDict()\n",
        "        for idx in range(1, self.hidden_layer_num+1):\n",
        "\n",
        "            # Affine layer\n",
        "            self.layers['Affine' + str(idx)] = Affine(self.params['W' + str(idx)],\n",
        "                                                      self.params['b' + str(idx)])\n",
        "            # Batch normalization 사용하는 경우\n",
        "            if self.use_batchnorm:\n",
        "                self.params['gamma' + str(idx)] = np.ones(hidden_size_list[idx-1])\n",
        "                self.params['beta' + str(idx)] = np.zeros(hidden_size_list[idx-1])\n",
        "                self.layers['BatchNorm' + str(idx)] = BatchNormalization(self.params['gamma' + str(idx)], self.params['beta' + str(idx)])\n",
        "            \n",
        "            # 활성화 함수 layer\n",
        "            self.layers['Activation_function' + str(idx)] = activation_layer[activation]()\n",
        "            \n",
        "            # Dropout\n",
        "            if self.use_dropout:\n",
        "                self.layers['Dropout' + str(idx)] = Dropout(dropout_ratio)\n",
        "\n",
        "        # 마지막 Affine layer\n",
        "        idx = self.hidden_layer_num + 1\n",
        "        self.layers['Affine' + str(idx)] = Affine(self.params['W' + str(idx)], self.params['b' + str(idx)])\n",
        "\n",
        "        # SoftmaxWithLoss layer\n",
        "        self.last_layer = SoftmaxWithLoss()\n",
        "\n",
        "\n",
        "    def predict(self, x, train_flg=False):\n",
        "        # 모든 layer에 대해서 순차적으로 forward\n",
        "        for key, layer in self.layers.items():\n",
        "            # Dropout과 BatchNorm의 경우, train과 test를 다르게 처리\n",
        "            if \"Dropout\" in key or \"BatchNorm\" in key:\n",
        "                x = layer.forward(x, train_flg)\n",
        "            else:\n",
        "                x = layer.forward(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def loss(self, x, t, train_flg=False):\n",
        "        # 순전파\n",
        "        y = self.predict(x, train_flg)\n",
        "\n",
        "        # Weight decay\n",
        "        weight_decay = 0\n",
        "        for idx in range(1, self.hidden_layer_num + 2):\n",
        "            W = self.params['W' + str(idx)]\n",
        "            weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W**2)\n",
        "\n",
        "        return self.last_layer.forward(y, t) + weight_decay\n",
        "\n",
        "    def accuracy(self, X, T):\n",
        "        # 순전파\n",
        "        Y = self.predict(X, train_flg=False)\n",
        "        # 출력값 중 가장 큰 값의 인덱스\n",
        "        Y = np.argmax(Y, axis=1)\n",
        "        # 정답 인덱스\n",
        "        if T.ndim != 1 : \n",
        "            T = np.argmax(T, axis=1)\n",
        "        # 정확도 \n",
        "        accuracy = np.sum(Y == T) / float(X.shape[0])\n",
        "        return accuracy\n",
        "\n",
        "       \n",
        "    def gradient(self, x, t):\n",
        "        # 순전파\n",
        "        self.loss(x, t, train_flg=True)\n",
        "\n",
        "        # 모든 layer에 대해서 역전파\n",
        "        dout = 1\n",
        "        dout = self.last_layer.backward(dout)\n",
        "\n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        # 구해진 기울기를 grads에 저장 후 return\n",
        "        grads = {}\n",
        "        for idx in range(1, self.hidden_layer_num+2):\n",
        "            grads['W' + str(idx)] = self.layers['Affine' + str(idx)].dW + self.weight_decay_lambda * self.params['W' + str(idx)]\n",
        "            grads['b' + str(idx)] = self.layers['Affine' + str(idx)].db\n",
        "\n",
        "            if self.use_batchnorm and idx != self.hidden_layer_num+1:\n",
        "                grads['gamma' + str(idx)] = self.layers['BatchNorm' + str(idx)].dgamma\n",
        "                grads['beta' + str(idx)] = self.layers['BatchNorm' + str(idx)].dbeta\n",
        "\n",
        "        return grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKU8VXeqPIgh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 6.9 \n",
        "\n",
        "x_train1 = x_train[:1000]\n",
        "t_train1 = t_train[:1000]\n",
        "\n",
        "max_epochs = 20\n",
        "train_size = x_train1.shape[0]\n",
        "batch_size = 100\n",
        "learning_rate = 0.01\n",
        "\n",
        "def _train(weight_init_std):\n",
        "    bn_network = MultiLayerNet(input_size=784, \n",
        "                                hidden_size_list=[100, 100, 100, 100, 100], \n",
        "                                output_size=10, weight_init_std=weight_init_std, use_batchnorm=True)\n",
        "    network = MultiLayerNet(input_size=784, \n",
        "                             hidden_size_list=[100, 100, 100, 100, 100], \n",
        "                             output_size¡=10, weight_init_std=weight_init_std)\n",
        "    optimizer = SGD(lr=learning_rate)\n",
        "    \n",
        "    train_acc_list = []\n",
        "    bn_train_acc_list = []\n",
        "    \n",
        "    iter_per_epoch = max(train_size / batch_size, 1)\n",
        "    epoch_cnt = 0\n",
        "    \n",
        "    for i in range(1000000000):\n",
        "        batch_mask = np.random.choice(train_size, batch_size)\n",
        "        x_batch = x_train1[batch_mask]\n",
        "        t_batch = t_train1[batch_mask]\n",
        "    \n",
        "        for _network in (bn_network, network):\n",
        "            grads = _network.gradient(x_batch, t_batch)\n",
        "            optimizer.update(_network.params, grads)\n",
        "    \n",
        "        if i % iter_per_epoch == 0:\n",
        "            train_acc = network.accuracy(x_train1, t_train1)\n",
        "            bn_train_acc = bn_network.accuracy(x_train1, t_train1)\n",
        "            train_acc_list.append(train_acc)\n",
        "            bn_train_acc_list.append(bn_train_acc)\n",
        "    \n",
        "            print(\"epoch:\" + str(epoch_cnt) + \" | \" + str(train_acc) + \" - \" + str(bn_train_acc))\n",
        "    \n",
        "            epoch_cnt += 1\n",
        "            if epoch_cnt >= max_epochs:\n",
        "                break\n",
        "                \n",
        "    return train_acc_list, bn_train_acc_list\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVUjidTiPIgj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 6.10\n",
        "\n",
        "weight_scale_list = np.logspace(0, -4, num=16)\n",
        "x = np.arange(max_epochs)\n",
        "\n",
        "plt.figure(figsize=(12,12))\n",
        "\n",
        "for i, w in enumerate(weight_scale_list):\n",
        "    print( \"============== \" + str(i+1) + \"/16\" + \" ==============\")\n",
        "    train_acc_list, bn_train_acc_list = _train(w)\n",
        "    \n",
        "    plt.subplot(4,4,i+1)\n",
        "    plt.title(\"W:\" + str(w))\n",
        "    if i == 15:\n",
        "        plt.plot(x, bn_train_acc_list, label='Batch Normalization', markevery=2)\n",
        "        plt.plot(x, train_acc_list, linestyle = \"--\", label='Normal(without BatchNorm)', markevery=2)\n",
        "    else:\n",
        "        plt.plot(x, bn_train_acc_list, markevery=2)\n",
        "        plt.plot(x, train_acc_list, linestyle=\"--\", markevery=2)\n",
        "\n",
        "    plt.ylim(0, 1.0)\n",
        "    if i % 4:\n",
        "        plt.yticks([])\n",
        "    else:\n",
        "        plt.ylabel(\"accuracy\")\n",
        "    if i < 12:\n",
        "        plt.xticks([])\n",
        "    else:\n",
        "        plt.xlabel(\"epochs\")\n",
        "    plt.legend(loc='lower right')\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zvh1gFrLPIgl",
        "colab_type": "text"
      },
      "source": [
        "### 바른 학습을 위해\n",
        "___"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEMQEzUePIgm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 6.11 Overfitting\n",
        "\n",
        "x_train1 = x_train[:300]\n",
        "t_train1 = t_train[:300]\n",
        "\n",
        "network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], output_size=10)\n",
        "optimizer = SGD(lr=0.01)\n",
        "\n",
        "max_epochs = 100\n",
        "train_size = x_train1.shape[0]\n",
        "batch_size = 100\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "epoch_cnt = 0\n",
        "\n",
        "for i in range(1000000000):\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train1[batch_mask]\n",
        "    t_batch = t_train1[batch_mask]\n",
        "\n",
        "    grads = network.gradient(x_batch, t_batch)\n",
        "    optimizer.update(network.params, grads)\n",
        "\n",
        "    if i % iter_per_epoch == 0:\n",
        "        train_acc = network.accuracy(x_train1, t_train1)\n",
        "        test_acc = network.accuracy(x_test, t_test)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "\n",
        "        print('epoch: {}, train acc: {}, test acc: {}'.format(epoch_cnt, train_acc, test_acc))\n",
        "\n",
        "        epoch_cnt += 1\n",
        "        if epoch_cnt >= max_epochs:\n",
        "            break\n",
        "\n",
        "\n",
        "# 그래프\n",
        "\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(max_epochs)\n",
        "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
        "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yVPEngEPIgo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 6.12 Weight decay\n",
        "\n",
        "x_train1 = x_train[:300]\n",
        "t_train1 = t_train[:300]\n",
        "\n",
        "weight_decay_lambda = 0.1\n",
        "\n",
        "network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], \n",
        "                              output_size=10, weight_decay_lambda=weight_decay_lambda)\n",
        "optimizer = SGD(lr=0.01)\n",
        "\n",
        "max_epochs = 100\n",
        "train_size = x_train1.shape[0]\n",
        "batch_size = 100\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "epoch_cnt = 0\n",
        "\n",
        "for i in range(1000000000):\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train1[batch_mask]\n",
        "    t_batch = t_train1[batch_mask]\n",
        "\n",
        "    grads = network.gradient(x_batch, t_batch)\n",
        "    optimizer.update(network.params, grads)\n",
        "\n",
        "    if i % iter_per_epoch == 0:\n",
        "        train_acc = network.accuracy(x_train1, t_train1)\n",
        "        test_acc = network.accuracy(x_test, t_test)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "\n",
        "        print('epoch: {}, train acc: {}, test acc: {}'.format(epoch_cnt, train_acc, test_acc))\n",
        "\n",
        "        epoch_cnt += 1\n",
        "        if epoch_cnt >= max_epochs:\n",
        "            break\n",
        "\n",
        "            \n",
        "# 그래프\n",
        "\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(max_epochs)\n",
        "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
        "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFeeAeK2PIgq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Trainer class\n",
        "\n",
        "class Trainer:\n",
        "\n",
        "    def __init__(self, network, x_train, t_train, x_test, t_test,\n",
        "                 epochs=20, mini_batch_size=100,\n",
        "                 optimizer='SGD', optimizer_param={'lr':0.01}, \n",
        "                 evaluate_sample_num_per_epoch=None, verbose=True):\n",
        "        \n",
        "        self.network = network\n",
        "        self.verbose = verbose\n",
        "        self.x_train = x_train\n",
        "        self.t_train = t_train\n",
        "        self.x_test = x_test\n",
        "        self.t_test = t_test\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = mini_batch_size\n",
        "        self.evaluate_sample_num_per_epoch = evaluate_sample_num_per_epoch\n",
        "\n",
        "        # optimizer\n",
        "        optimizer_class_dict = {'sgd':SGD, 'momentum':Momentum, 'nesterov':Nesterov,\n",
        "                                'adagrad':AdaGrad, 'rmsprpo':RMSprop, 'adam':Adam}\n",
        "        self.optimizer = optimizer_class_dict[optimizer.lower()](**optimizer_param)\n",
        "        \n",
        "        self.train_size = x_train.shape[0]\n",
        "        self.iter_per_epoch = max(self.train_size / mini_batch_size, 1)\n",
        "        self.max_iter = int(epochs * self.iter_per_epoch)\n",
        "        self.current_iter = 0\n",
        "        self.current_epoch = 0\n",
        "        \n",
        "        self.train_loss_list = []\n",
        "        self.train_acc_list = []\n",
        "        self.test_acc_list = []\n",
        "\n",
        "    def train_step(self):\n",
        "        batch_mask = np.random.choice(self.train_size, self.batch_size)\n",
        "        x_batch = self.x_train[batch_mask]\n",
        "        t_batch = self.t_train[batch_mask]\n",
        "        \n",
        "        grads = self.network.gradient(x_batch, t_batch)\n",
        "        self.optimizer.update(self.network.params, grads)\n",
        "        \n",
        "        loss = self.network.loss(x_batch, t_batch)\n",
        "        self.train_loss_list.append(loss)\n",
        "        if self.verbose: print(\"train loss:\" + str(loss))\n",
        "        \n",
        "        if self.current_iter % self.iter_per_epoch == 0:\n",
        "            self.current_epoch += 1\n",
        "            \n",
        "            x_train_sample, t_train_sample = self.x_train, self.t_train\n",
        "            x_test_sample, t_test_sample = self.x_test, self.t_test\n",
        "            if not self.evaluate_sample_num_per_epoch is None:\n",
        "                t = self.evaluate_sample_num_per_epoch\n",
        "                x_train_sample, t_train_sample = self.x_train[:t], self.t_train[:t]\n",
        "                x_test_sample, t_test_sample = self.x_test[:t], self.t_test[:t]\n",
        "                \n",
        "            train_acc = self.network.accuracy(x_train_sample, t_train_sample)\n",
        "            test_acc = self.network.accuracy(x_test_sample, t_test_sample)\n",
        "            self.train_acc_list.append(train_acc)\n",
        "            self.test_acc_list.append(test_acc)\n",
        "\n",
        "            if self.verbose: print(\"=== epoch:\" + str(self.current_epoch) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc) + \" ===\")\n",
        "        self.current_iter += 1\n",
        "\n",
        "    def train(self):\n",
        "        for i in range(self.max_iter):\n",
        "            self.train_step()\n",
        "\n",
        "        test_acc = self.network.accuracy(self.x_test, self.t_test)\n",
        "\n",
        "        if self.verbose:\n",
        "            print(\"=============== Final Test Accuracy ===============\")\n",
        "            print(\"test acc:\" + str(test_acc))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8X9ocmRPIgs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 6.13 Dropout\n",
        "\n",
        "class Dropout:\n",
        "\n",
        "    def __init__(self, dropout_ratio=0.5):\n",
        "        self.dropout_ratio = dropout_ratio\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x, train_flg=True):\n",
        "        if train_flg:\n",
        "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
        "            return x * self.mask\n",
        "        else:\n",
        "            return x * (1.0 - self.dropout_ratio)\n",
        "\n",
        "    def backward(self, dout):\n",
        "        return dout * self.mask\n",
        "\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mp_RBajePIgt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 6.14\n",
        "\n",
        "x_train1 = x_train[:300]\n",
        "t_train1 = t_train[:300]\n",
        "\n",
        "use_dropout = True\n",
        "dropout_ratio = 0.2\n",
        "\n",
        "network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], \n",
        "                              output_size=10, use_dropout=use_dropout, dropout_ratio=dropout_ratio)\n",
        "\n",
        "trainer = Trainer(network, x_train1, t_train1, x_test, t_test,\n",
        "                  epochs=301, mini_batch_size=100,\n",
        "                  optimizer='sgd', optimizer_param={'lr': 0.01}, verbose=True)\n",
        "trainer.train()\n",
        "\n",
        "train_acc_list, test_acc_list = trainer.train_acc_list, trainer.test_acc_list\n",
        "\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(len(train_acc_list))\n",
        "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
        "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogGEJkIlPIgv",
        "colab_type": "text"
      },
      "source": [
        "### 하이퍼파라미터 튜닝\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQgCMZEBPIgv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def shuffle_dataset(x, t):\n",
        "\n",
        "    permutation = np.random.permutation(x.shape[0])\n",
        "    x = x[permutation,:] if x.ndim == 2 else x[permutation,:,:,:]\n",
        "    t = t[permutation]\n",
        "\n",
        "    return x, t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85FBHoXtPIgy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 6.15 검증 데이터\n",
        "\n",
        "# 훈련 데이터를 섞는다.\n",
        "x_train1, t_train1 = shuffle_dataset(x_train, t_train)\n",
        "\n",
        "# 20%를 검증 데이터로 분할\n",
        "validation_rate = 0.20\n",
        "validation_num = int(x_train1.shape[0] * validation_rate)\n",
        "\n",
        "x_val = x_train1[ : validation_num]\n",
        "t_val = t_train1[ : validation_num]\n",
        "x_train1 = x_train1[validation_num : ]\n",
        "t_train1 = t_train1[validation_num : ]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtMxC0mvPIg0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 6.16 가중치 감소 계수와 학습율\n",
        "\n",
        "weight_decay = 10 ** np.random.uniform(-8, -4)\n",
        "lr = 10 ** np.random.uniform(-6, -2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXPNzirkPIg2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 6.17\n",
        "\n",
        "\n",
        "x_train1 = x_train[:500]\n",
        "t_train1 = t_train[:500]\n",
        "\n",
        "x_train1, t_train1 = shuffle_dataset(x_train1, t_train1)\n",
        "\n",
        "validation_rate = 0.20\n",
        "validation_num = int(x_train1.shape[0] * validation_rate)\n",
        "\n",
        "x_val = x_train1[:validation_num]\n",
        "t_val = t_train1[:validation_num]\n",
        "x_train1 = x_train1[validation_num:]\n",
        "t_train1 = t_train1[validation_num:]\n",
        "\n",
        "\n",
        "def _train(lr, weight_decay, epocs=50):\n",
        "    network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
        "                            output_size=10, weight_decay_lambda=weight_decay)\n",
        "    trainer = Trainer(network, x_train1, t_train1, x_val, t_val,\n",
        "                      epochs=epocs, mini_batch_size=100,\n",
        "                      optimizer='sgd', optimizer_param={'lr': lr}, verbose=False)\n",
        "    trainer.train()\n",
        "\n",
        "    return trainer.test_acc_list, trainer.train_acc_list\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SbuL8fPPIg4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 6.18\n",
        "\n",
        "optimization_trial = 100\n",
        "results_val = {}\n",
        "results_train = {}\n",
        "\n",
        "for _ in range(optimization_trial):\n",
        "    weight_decay = 10 ** np.random.uniform(-8, -4)\n",
        "    lr = 10 ** np.random.uniform(-6, -2)\n",
        "    # ================================================\n",
        "    val_acc_list, train_acc_list = _train(lr, weight_decay)\n",
        "    print(\"val acc: {} | lr: {}, weight decay: {}\".format(val_acc_list[-1], lr, weight_decay))\n",
        "    key = \"lr:\" + str(lr) + \", weight decay:\" + str(weight_decay)\n",
        "    results_val[key] = val_acc_list\n",
        "    results_train[key] = train_acc_list\n",
        "\n",
        "print(\"=========== Hyper-Parameter Optimization Result ===========\")\n",
        "graph_draw_num = 20\n",
        "col_num = 5\n",
        "row_num = int(np.ceil(graph_draw_num / col_num))\n",
        "i = 0\n",
        "\n",
        "for key, val_acc_list in sorted(results_val.items(), key=lambda x:x[1][-1], reverse=True):\n",
        "    print(\"Best-\" + str(i+1) + \"(val acc:\" + str(val_acc_list[-1]) + \") | \" + key)\n",
        "\n",
        "    plt.subplot(row_num, col_num, i+1)\n",
        "    plt.title(\"Best-\" + str(i+1))\n",
        "    plt.ylim(0.0, 1.0)\n",
        "    if i % 5: plt.yticks([])\n",
        "    plt.xticks([])\n",
        "    x = np.arange(len(val_acc_list))\n",
        "    plt.plot(x, val_acc_list)\n",
        "    plt.plot(x, results_train[key], \"--\")\n",
        "    i += 1\n",
        "\n",
        "    if i >= graph_draw_num:\n",
        "        break\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jitNHd_kPIg6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HHZaSqUPIg8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}